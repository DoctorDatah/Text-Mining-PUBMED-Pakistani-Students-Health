library(tm)
library(tidytext)          #For sentiments
library(dplyr)             #For joins
library(wordcloud)
library(wordcloud2)
library(phm)               #Phrase mining
###################################################################################
# Loading Custom Libraries
###################################################################################
source("lib/toDataFrame.R")
source("lib/PubMed_Utils.R")
source("lib/PhraseMining_Utils.R")
###################################################################################
# Save the environment
###################################################################################
parSave=par(no.readonly = TRUE)
###################################################################################
# Loading Data
###################################################################################
FILE_PATH = "Data/pubmed-pakistanis-set.txt"
(co=VCorpus(PubMedSource(toDataFrame(FILE_PATH))))
###################################################################################
# Phrases
###################################################################################
# Creating
pd = phraseDoc(co, min.freq=3,silent=T)
pdm = as.matrix(pd)
pdm[1:8,1:8]
# All Principal Phrases
row.names(pdm)
# High Frequency Phrases
(highFreq=freqPhrases(pd,5))
# Stop Words
NAMES_PATH = "Data/names.csv"
OTHER_PHRASES_PATH = "Data/other_phrases.csv"
NAMES = read.csv(NAMES_PATH)$Names
OTHER_PHRASES = read.csv(OTHER_PHRASES_PATH)$Phrases
STOP_PHRASES = c(NAMES,OTHER_PHRASES)
STOP_PHRASES
pd = phraseDoc(co,min.freq=3,silent=T,sp = STOP_PHRASES)
pdm = as.matrix(pd)
pdm[1:8,1:8]
# High Frequency Phrases
(highFreq=freqPhrases(pd,5))
###################################################################################
# Clustering
###################################################################################
set.seed(1)
k=3  #Number of clusters
km = kmeans(t(pdm), k)
km$cluster
km$size
#Most frequent phrases in clusters (note: average frequencies!)
# cluster 1
(clust1_hfPfrases = head(sort(km$centers[1,],decreasing=T),100))
#Most frequent phrases in clusters (note: average frequencies!)
# cluster 1
clust1_hfPfrases = head(sort(km$centers[1,],decreasing=T),100)
df = data.frame(word=names(clust1_hfPfrases),freq=clust1_hfPfrases)
wordcloud2(df,size=.2, shape='cloud')
# cluster 2
clust2_hfPfrases = head(sort(km$centers[2,],decreasing=T),50)
df = data.frame(word=names(clust2_hfPfrases),freq=clust2_hfPfrases)
wordcloud2(df,size=.2, shape='cloud')
# cluster 3
clust3_hfPfrases = head(sort(km$centers[3,],decreasing=T),50)
df = data.frame(word=names(clust3_hfPfrases),freq=clust3_hfPfrases)
wordcloud2(df,size=.2, shape='cloud')
km$size
###################################################################################
# Checking the documents contain Phrase breast cancer
###################################################################################
# Finding docs
phraseToFind=c("breast cancer")
(docBC_ids = get_docIDs_from_phrases(pd,phraseToFind))
# Titles
print_title_of_docs(co,docBC_ids)
# Titles
unlist(print_title_of_docs(co,docBC_ids))
# Titles
unlist(print_title_of_docs(co,docBC_ids))
# Content
unlist(print_content_of_docs(co,docBC_ids))
lapply*=(co[docBC_ids],meta,"title")
lapply=(co[docBC_ids],meta,"title")
lapply(co[docBC_ids],meta,"title")
# Titles
unlist(lapply(co[docBC_ids],meta,"title"))
# Content
lapply(co[docBC_ids],meta,"inspect")
# Content
lapply(co[docBC_ids],meta,"content")
# Content
lapply(co[docBC_ids],inspect)
# Content
lapply(co[docBC_ids],content)
# Titles
lapply(co[docBC_ids],meta,"title")
lapply(co[docBC_ids],meta,"id")
lapply(co[docBC_ids],meta)
lapply(co[docBC_ids],meta)$id
lapply(co[docBC_ids],meta)
# Titles
lapply(co[docBC_ids],meta,"title")
#Document indices  cluster 3
idx3 = which(km$cluster==3)
(co3 = co[idx3])
pd3 = phraseDoc(co3, min.freq=3,silent=T,sp = STOP_PHRASES)
pdm3 = as.matrix(pd3)
# High Frequency Phrases
(highFreq_c3=freqPhrases(pd3,5))
# cluster 3
clust3_hfPfrases = head(sort(km$centers[3,],decreasing=T),50)
df = data.frame(word=names(clust3_hfPfrases),freq=clust3_hfPfrases)
wordcloud2(df,size=.2, shape='cloud')
# Indics for all cluster
idx1 = which(km$cluster==1)
idx2 = which(km$cluster==2)
idx3 = which(km$cluster==3)
pd[docBC_ids]
?freqPhrases
pd
hf_BC_phrases = freqPhrases(co[docBC_ids])
docBC_ids
pdm[docBC_ids]
pd[,docBC_ids]
pdm[,docBC_ids]
hf_BC_phrases = freqPhrases(pdm[,docBC_ids])
pd3
###################################################################################
# Project: Pakistani Students Health papers form PubMed
# By: Malik Hassan Qayyum
###################################################################################
library(readtext)
library(tm)
library(tidytext)          #For sentiments
library(dplyr)             #For joins
library(wordcloud)
library(wordcloud2)
library(phm)               #Phrase mining
###################################################################################
# Loading Custom Libraries
###################################################################################
source("lib/toDataFrame.R")
source("lib/PubMed_Utils.R")
source("lib/PhraseMining_Utils.R")
###################################################################################
# Save the environment
###################################################################################
parSave=par(no.readonly = TRUE)
###################################################################################
# Loading Data
###################################################################################
FILE_PATH = "Data/pubmed-pakistanis-set.txt"
(co=VCorpus(PubMedSource(toDataFrame(FILE_PATH))))
###################################################################################
# Phrases
###################################################################################
# Creating
pd = phraseDoc(co, min.freq=3,silent=T)
pdm = as.matrix(pd)
pdm[1:8,1:8]
# All Principal Phrases
row.names(pdm)
# High Frequency Phrases
(highFreq=freqPhrases(pd,5))
# Stop Words
NAMES_PATH = "Data/names.csv"
OTHER_PHRASES_PATH = "Data/other_phrases.csv"
NAMES = read.csv(NAMES_PATH)$Names
OTHER_PHRASES = read.csv(OTHER_PHRASES_PATH)$Phrases
STOP_PHRASES = c(NAMES,OTHER_PHRASES)
pd = phraseDoc(co,min.freq=3,silent=T,sp = STOP_PHRASES)
pdm = as.matrix(pd)
pdm[1:8,1:8]
# High Frequency Phrases
(highFreq=freqPhrases(pd,5))
###################################################################################
# Clustering
###################################################################################
set.seed(1)
k=3  #Number of clusters
km = kmeans(t(pdm), k)
km$cluster
km$size
#Most frequent phrases in clusters (note: average frequencies!)
# cluster 1
clust1_hfPfrases = head(sort(km$centers[1,],decreasing=T),100)
df = data.frame(word=names(clust1_hfPfrases),freq=clust1_hfPfrases)
wordcloud2(df,size=.2, shape='cloud')
# cluster 2
clust2_hfPfrases = head(sort(km$centers[2,],decreasing=T),50)
df = data.frame(word=names(clust2_hfPfrases),freq=clust2_hfPfrases)
wordcloud2(df,size=.2, shape='cloud')
# cluster 3
clust3_hfPfrases = head(sort(km$centers[3,],decreasing=T),50)
df = data.frame(word=names(clust3_hfPfrases),freq=clust3_hfPfrases)
wordcloud2(df,size=.2, shape='cloud')
# Indics for all cluster
idx1 = which(km$cluster==1)
idx2 = which(km$cluster==2)
idx3 = which(km$cluster==3)
idx3
idx2
# Indics for all cluster
idx1 = as.integer(which(km$cluster==1))
idx1
idx3 = as.integer(which(km$cluster==3))
idx3
idx2
idx2 = as.integer(which(km$cluster==2))
idx2
###################################################################################
# Checking the documents contain Phrase breast cancer
###################################################################################
# Finding docs
(docBC_ids = get_docIDs_from_phrases(pd,"breast cancer"))
# Titles
lapply(co[docBC_ids],meta,"title")
lapply(co[docBC_ids],meta)
dim(pd)
pd
###################################################################################
# Checking the documents contain Phrase breast cancer
###################################################################################
# Finding docs
docsBC = getDocs(pd,"breast cancer")
docBC_ids =as.integer(names(docsBC[,]))
docBC_ids
(docBC_ids =as.integer(names(docsBC[,])))
# Titles
lapply(co[docBC_ids],meta,"title")
lapply(co[docBC_ids],meta)
docsBC
docBC_ids
co[docBC_ids]
docsBC
(docBC_ids =as.integer(names(docsBC[,])))
co[docBC_ids]
pd_BC = phraseDoc(co[docBC_ids], min.freq = 3)
pdm_BC = as.matrix(pd_BC)
dim(pdm_BC)
# Titles
lapply(co[docBC_ids],meta,"title")
lapply(co[docBC_ids],meta)
hf_BC_phrases = freqPhrases(pd_BC, 50)
# HF Phrases
hf_BC_phrases = freqPhrases(pd_BC, 50)
df = data.frame(word=names(hf_BC_phrases),freq=hf_BC_phrases)
wordcloud2(df,size=.2, shape='cloud')
pd_BC = phraseDoc(co[docBC_ids], min.freq = 3,sp=STOP_PHRASES)
pdm_BC = as.matrix(pd_BC)
dim(pdm_BC)
# Titles
lapply(co[docBC_ids],meta,"title")
lapply(co[docBC_ids],meta)
# HF Phrases
hf_BC_phrases = freqPhrases(pd_BC, 50)
df = data.frame(word=names(hf_BC_phrases),freq=hf_BC_phrases)
wordcloud2(df,size=.2, shape='cloud')
wordcloud2(df,size=.3, shape='cloud')
###################################################################################
# Continuing with cluster 2
# Plotting Phrases frequency
###################################################################################
#Obtain the total frequencies by phrase
sums=apply(pdm,1,sum)
###################################################################################
# Continuing with cluster 2
# Plotting Phrases frequency
###################################################################################
#Obtain the total frequencies by phrase
sums=apply(pdm2,1,sum)
# Cluster 2 seems the cluster of my interest
(co2 = co[idx2])
pd2 = phraseDoc(co2, min.freq=3,silent=T,sp = STOP_PHRASES)
pdm2 = as.matrix(pd2)
dim(pdm2)
###################################################################################
# Checking the documents contain Phrase breast cancer
###################################################################################
# Finding docs
docsBC = getDocs(pd,"breast cancer")
(docBC_ids =as.integer(names(docsBC[,])))
pd_BC = phraseDoc(co[docBC_ids], min.freq = 3,sp=STOP_PHRASES)
pdm_BC = as.matrix(pd_BC)
dim(pdm_BC)
# Titles
lapply(co[docBC_ids],meta,"title")
lapply(co[docBC_ids],meta)
# HF Phrases
hf_BC_phrases = freqPhrases(pd_BC, 50)
df = data.frame(word=names(hf_BC_phrases),freq=hf_BC_phrases)
wordcloud2(df,size=.3, shape='cloud')
###################################################################################
# Continuing with cluster 2
# Plotting Phrases frequency
###################################################################################
#Obtain the total frequencies by phrase
sums=apply(pdm2,1,sum)
dim(pdm) # no. of phrases
sums
# Getting only top 100 phrases
top100Phrases=freqPhrases(pd,100)
top100Phrases
###################################################################################
# Continuing with cluster 2
# Plotting Phrases frequency
###################################################################################
# Obtain the total frequencies by phrase
sums=apply(pdm2,1,sum)
sums
top100Phrases
# Getting only top 100 phrases
top100Phrases=freqPhrases(pd,100)
#Word cloud2
df = data.frame(word=names(top100Phrases),freq=top100Phrases)
wordcloud2(df,size=.2, shape='cloud')
colFreq=apply(pdm,2,sum)
sum(colFreq==0)
bd=bestDocs(co,nDocs = 10,nPhrases = 5, STOP_PHRASES)
bd=bestDocs(co2,nDocs = 10,nPhrases = 5, STOP_PHRASES)
bd
source("lib/PubMed_Utils.R")
source("lib/PhraseMining_Utils.R")
# function
topDocIndicsAndFreq = get_doc_oldID_and_num_hfphrases(c0 =bd)
# function
topDocIndicsAndFreq = get_doc_oldID_and_num_hfphrases(co =bd)
topDocIndicsAndFreq
topDocIndicsAndFreq[2]
(topDocIndicsAndFreq = get_doc_oldID_and_num_hfphrases(co=bd))
topDocIndicsAndFreq[2]
oldID[1]
topDocIndicsAndFreq[1]
(id = topDocIndicsAndFreq[1]) # __ index
topDocIndicsAndFreq[1,2] # __ hf phrases
topDocIndicsAndFreq[,2] # __ hf phrases
topDocIndicsAndFreq[2,2] # __ hf phrases
topDocIndicsAndFreq[1,] # __ hf phrases
topDocIndicsAndFreq
topDocIndicsAndFreq[1,]$oldID
topDocIndicsAndFreq[1,]
###################################################################################
# let's proceed with cluster number 2
# Best Document
###################################################################################
## 5 High Frequency Phrases
bd=bestDocs(co2,nDocs = 10,nPhrases = 5, STOP_PHRASES)
(topDocIndicsAndFreq = get_doc_oldID_and_num_hfphrases(co=bd))
(id = topDocIndicsAndFreq[1])
meta(co[[id]])
# Checking this document in original cluster 2 corpus
meta(co[[id]])
inspect(co2[[id]])
# All principal phrases
pdm[pdm[,id] != 0,id, drop=F]
# All principal phrases
pdm[pdm[,id] != 0,id, drop=F]
pdm[pdm[,id] != 0,id, drop=F]
pdm[pdm[,id] != 0,id, drop=F]
df = data.frame(word=names(hf_BC_phrases),freq=hf_BC_phrases)
wordcloud2(df,size=.3, shape='cloud')
# comapring to top high frequency phrases
freqPhrases(pd,5)
# All principal phrases
sort(pdm[pdm[,id] != 0,id, drop=F])
# All principal phrases
pdm[pdm[,id] != 0,id, drop=F]
# All principal phrases
phrases = pdm[pdm[,id] != 0,id, drop=F]
phrases
phrases$phrases
names(phrases)
# All principal phrases
phrases = pdm[pdm[,id] != 0,id, drop=F]
row.names(phrases)
df = data.frame(word=row.names(phrases),freq=phrases)
phrases
class(phrases)
df = data.frame(word=row.names(phrases),freq=phrases)
wordcloud2(df,size=.3, shape='cloud')
# comapring to top high frequency phrases
freqPhrases(pd,5)
###################################################################################
## 10 High Frequency Phrases
bd=bestDocs(co2,nDocs = 10,nPhrases = 5, STOP_PHRASES)
(topDocIndicsAndFreq = get_doc_oldID_and_num_hfphrases(co=bd))
(id = topDocIndicsAndFreq[1])
###################################################################################
## 10 High Frequency Phrases
bd=bestDocs(co2,nDocs = 10,nPhrases = 10, STOP_PHRASES)
(topDocIndicsAndFreq = get_doc_oldID_and_num_hfphrases(co=bd))
(id = topDocIndicsAndFreq[1])
# Checking this document in original cluster 2 corpus
meta(co[[id]])
(id = topDocIndicsAndFreq[2])
# Checking this document in original cluster 2 corpus
meta(co[[id]])
inspect(co2[[id]])
# All principal phrases
phrases = pdm[pdm[,id] != 0,id, drop=F]
class(phrases)
df = data.frame(word=row.names(phrases),freq=phrases)
wordcloud2(df,size=.3, shape='cloud')
phrases
###################################################################################
## 10 High Frequency Phrases
bd=bestDocs(co2,nDocs = 10,nPhrases = 15, STOP_PHRASES)
(topDocIndicsAndFreq = get_doc_oldID_and_num_hfphrases(co=bd))
(id = topDocIndicsAndFreq[2])
(topDocIndicsAndFreq = get_doc_oldID_and_num_hfphrases(co=bd))
###################################################################################
## 10 High Frequency Phrases
bd=bestDocs(co2,nDocs = 10,nPhrases = 20, STOP_PHRASES)
(topDocIndicsAndFreq = get_doc_oldID_and_num_hfphrases(co=bd))
###################################################################################
## 10 High Frequency Phrases
bd=bestDocs(co2,nDocs = 10,nPhrases = 10, STOP_PHRASES)
(topDocIndicsAndFreq = get_doc_oldID_and_num_hfphrases(co=bd))
(id = topDocIndicsAndFreq[1])
# Same Document at the Top
ChosenDocID = id
ptd=co2[[ChosenDocID]]    # document in the corpus
###################################################################################
# Sentiment Analysis of the Doc
###################################################################################
# from cluster 2 corpus
ptd=co2[[ChosenDocID]]    # document in the corpus
ptd
lapply(bd[length(bd)], meta,"oldID")
lapply(bd[1:length(bd)], meta,"oldID")
bd
lapply(bd[1:length(bd)], meta,"oldID")
names(lapply(bd[1:length(bd)], meta,"oldID"))
as.integer(names(lapply(bd[1:length(bd)], meta,"oldID")))
oldID = as.integer(names(lapply(bd[1:length(bd)], meta,"oldID")))
oldID
lapply(bd[1:length(bd)], meta,"oldID"))
lapply(bd[1:length(bd)], meta,"oldID")
oldID = as.integer((lapply(bd[1:length(bd)], meta,"oldID"))
oldID = as.integer((lapply(bd[1:length(bd)], meta,"oldID")))
oldID
source('~/GitHub/Network-and-Text-Analytics-/Project Text Mining/main.R', echo=TRUE)
oldID = as.integer((lapply(bd[1:length(bd)], meta,"oldID")))
oldID
hfPhrases = as.integer((lapply(bd[1:length(bd)], meta,"hfPhrases")))
oldID = as.integer((lapply(bd[1:length(bd)], meta,"oldID")))
hfPhrases = as.integer((lapply(bd[1:length(bd)], meta,"hfPhrases")))
topDocIndicsAndFreq = cbind(oldID,hfPhrases)
(topDocIndicsAndFreq = cbind(oldID,hfPhrases))
}
###################################################################################
get_doc_oldID_and_num_hfphrases = function(co){
oldID = as.integer((lapply(bd[1:length(bd)], meta,"oldID")))
hfPhrases = as.integer((lapply(bd[1:length(bd)], meta,"hfPhrases")))
topDocIndicsAndFreq = cbind(oldID,hfPhrases)
return(topDocIndicsAndFreq)
}
###################################################################################
## 10 High Frequency Phrases
bd=bestDocs(co2,nDocs = 10,nPhrases = 10, STOP_PHRASES)
(topDocIndicsAndFreq = get_doc_oldID_and_num_hfphrases(co=bd))
source("lib/PhraseMining_Utils.R")
(topDocIndicsAndFreq = get_doc_oldID_and_num_hfphrases(co=bd))
# Same Document at the Top
ChosenDocID = topDocIndicsAndFreq[1]
ChosenDocID
###################################################################################
# Sentiment Analysis of the Doc
###################################################################################
# From cluster 2 corpus
(ptd_f =co2[[ChosenDocID]])
tf=termFreq(ptd,control=list(removePunctuation=T,stopwords=T,removeNumbers=T))
tf
library(sur)               #Skewness ratios
#par(parSave)
###################################################################################
# Processing
###################################################################################
#1
co=VCorpus(PubMedSource(toDataFrame("TextData/tmData/Covid19March1.txt")))
#a
tdM=as.matrix(TermDocumentMatrix(co, control = list(removePunctuation = TRUE,
stopwords = T,
removeNumbers = TRUE)))
###################################################################################
#
# HW11.R
#
###################################################################################
# External Functions
###################################################################################
library(tm)
library(tidytext)          #For sentiments
library(dplyr)             #For joins
library(wordcloud)
library(wordcloud2)
library(sur)               #Skewness ratios
source("R-Code/Text Analysis/toDataFrame.R")
###################################################################################
# Internal Functions
###################################################################################
PubMedSource<-function (x)
{
stopifnot(all(!is.na(match(c("PMID", "Title","Abstract","Date","Author"),
names(x)))))
SimpleSource(length = nrow(x), reader = readPub, content = x,
class = "PubMedSource")
}
source("R-Code/Text Analysis/toDataFrame.R")
source("R-Code/Text Analysis/toDataFrame.R")
